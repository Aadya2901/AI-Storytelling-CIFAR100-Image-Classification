{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aadya2901/AI-Storytelling-CIFAR100-Image-Classification/blob/main/Vision_AI_Fundamentals_Building_a_Digit_Recognizer_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f3e26de"
      },
      "source": [
        "# Fashion-MNIST Classification Project\n",
        "\n",
        "This project aims to build and compare different deep learning models for classifying images from the Fashion-MNIST dataset. We will explore the impact of increasing model complexity, from a simple Artificial Neural Network (ANN) to more sophisticated Convolutional Neural Networks (CNNs), on performance and efficiency.\n",
        "\n",
        "**Project Agenda & Steps:**\n",
        "\n",
        "1.  **Dataset Setup:**\n",
        "    *   Import necessary libraries.\n",
        "    *   Load the Fashion-MNIST dataset.\n",
        "    *   Preprocess the data (normalize and reshape images, one-hot encode labels).\n",
        "    *   Verify the shapes of the processed data.\n",
        "\n",
        "2.  **Model Building:**\n",
        "    *   Define the architecture for each model:\n",
        "        *   Basic ANN Model\n",
        "        *   Basic CNN Model\n",
        "        *   Deeper CNN Model\n",
        "\n",
        "3.  **Model Training:**\n",
        "    *   Train each model using the prepared training data.\n",
        "    *   Implement Early Stopping and Model Checkpointing to optimize training and save the best model weights.\n",
        "\n",
        "4.  **Model Evaluation:**\n",
        "    *   Load the best weights for each trained model.\n",
        "    *   Evaluate each model's performance on the test set using metrics like loss and accuracy.\n",
        "    *   Visualize the training history (accuracy and loss curves) for comparison.\n",
        "    *   Generate and visualize confusion matrices to understand model performance on each class.\n",
        "\n",
        "5.  **Prediction Analysis:**\n",
        "    *   Use the best performing model (Basic CNN in this case) to make predictions on the test set.\n",
        "    *   Identify and visualize examples of both correctly and incorrectly classified images to gain insights into model behavior.\n",
        "\n",
        "**Goal:** To analyze and demonstrate how model complexity influences classification accuracy and efficiency on the Fashion-MNIST dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Dataset Setup Documentation"
      ],
      "metadata": {
        "id": "GtKuH4KkTtE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1) Library Imports\n",
        "\n",
        "This section imports the essential libraries:  \n",
        "- **NumPy** for numerical operations  \n",
        "- **TensorFlow / Keras** for building and training deep learning models  \n",
        "- **Matplotlib** for visualizations  "
      ],
      "metadata": {
        "id": "JRkB4PzvYkFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots"
      ],
      "metadata": {
        "id": "3EB8qWlpT97a"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2) Load Dataset\n",
        "\n",
        "Load the **Fashion-MNIST** dataset from Keras, splitting it into:  \n",
        "- `train_images`, `train_labels` → training set  \n",
        "- `test_images`, `test_labels` → test set  \n"
      ],
      "metadata": {
        "id": "lDdiceLEYuEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLhqm_OwT-27",
        "outputId": "2135b6b7-b0b4-4aa9-894c-8965b29345a6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3) Normalize Data\n",
        "\n",
        "Scale pixel values from **0–255** to the range **0–1** for faster and more stable training.  \n",
        "\n",
        "**Example:**  \n",
        "- Before: a pixel value could be `200`  \n",
        "- After: `200 / 255 ≈ 0.78`  \n"
      ],
      "metadata": {
        "id": "gWFb4KiKYz8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "metadata": {
        "id": "VpBVd73PUARF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4) Reshape Data\n",
        "\n",
        "Reshape images to **28×28×1** by adding a channel dimension, making them compatible with CNN layers.  \n",
        "\n",
        "**Example:**  \n",
        "- Before: `(60000, 28, 28)` → 60,000 grayscale images without channel info  \n",
        "- After: `(60000, 28, 28, 1)` → channel dimension (`1`) added for CNN input  \n"
      ],
      "metadata": {
        "id": "eARU4Ml3Y7ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))\n",
        "test_images = test_images.reshape((test_images.shape[0], 28, 28, 1))"
      ],
      "metadata": {
        "id": "0YPN1XfvUDhW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.5) One-Hot Encode Labels\n",
        "\n",
        "Convert class labels (0–9) into **one-hot encoded vectors** with 10 output classes, required for multi-class classification.  \n",
        "\n",
        "**Example:**  \n",
        "- Original label: `3`  \n",
        "- One-hot encoded: `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`  \n"
      ],
      "metadata": {
        "id": "HjRlf9-EZDKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels_one_hot = keras.utils.to_categorical(train_labels, num_classes=10)\n",
        "test_labels_one_hot = keras.utils.to_categorical(test_labels, num_classes=10)"
      ],
      "metadata": {
        "id": "IXAUOyMoUErl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.6) Verify Dataset Shapes\n",
        "\n",
        "Print the shapes of images and labels after preprocessing to confirm everything is in the expected format.  \n",
        "\n",
        "**Example output:**  \n",
        "- Training images shape: `(60000, 28, 28, 1)`  \n",
        "- Testing images shape: `(10000, 28, 28, 1)`  \n",
        "- Training labels shape: `(60000, 10)`  \n",
        "- Testing labels shape: `(10000, 10)`  \n"
      ],
      "metadata": {
        "id": "tDoWjyciZUBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training images shape:\", train_images.shape)\n",
        "print(\"Testing images shape:\", test_images.shape)\n",
        "print(\"Training labels shape:\", train_labels_one_hot.shape)\n",
        "print(\"Testing labels shape:\", test_labels_one_hot.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7-bd9rhUFwj",
        "outputId": "74dd7463-9dff-4b7f-8e8e-105f39933b6f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images shape: (60000, 28, 28, 1)\n",
            "Testing images shape: (10000, 28, 28, 1)\n",
            "Training labels shape: (60000, 10)\n",
            "Testing labels shape: (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa61470f"
      },
      "source": [
        "### 2. Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1) Basic ANN Model"
      ],
      "metadata": {
        "id": "i53jkqYtUXwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Flatten (28×28→784)** → 0 params  \n",
        "- **Dense (128, ReLU)** → 100,480 params  \n",
        "- **Dense (64, ReLU)** → 8,256 params  \n",
        "- **Dense (10, Softmax)** → 650 params  \n",
        "\n",
        "**Total params:** ~109K  "
      ],
      "metadata": {
        "id": "yoPgGlRReUBx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "9c0017b8",
        "outputId": "69292a31-4c09-4406-82dc-f49f49d51555"
      },
      "source": [
        "ann_model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "ann_model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "ann_model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m109,386\u001b[0m (427.29 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,386</span> (427.29 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m109,386\u001b[0m (427.29 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,386</span> (427.29 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d5f1731"
      },
      "source": [
        "#### 2.2) Basic CNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Conv2D (32, 3×3) + MaxPool** → 320 params  \n",
        "- **Conv2D (64, 3×3) + MaxPool** → 18,496 params  \n",
        "- **Flatten**  \n",
        "- **Dense (64, ReLU)** → 102,464 params  \n",
        "- **Dense (10, Softmax)** → 650 params  \n",
        "\n",
        "**Total params:** ~121K  "
      ],
      "metadata": {
        "id": "RoLST4EzeWbK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "14857787",
        "outputId": "7bfd9c6e-02f7-4c93-8f44-3375dcbfa8b0"
      },
      "source": [
        "basic_cnn_model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "basic_cnn_model.compile(optimizer='adam',\n",
        "                        loss='categorical_crossentropy',\n",
        "                        metrics=['accuracy'])\n",
        "\n",
        "basic_cnn_model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │       \u001b[38;5;34m102,464\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">102,464</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m121,930\u001b[0m (476.29 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">121,930</span> (476.29 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m121,930\u001b[0m (476.29 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">121,930</span> (476.29 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe703f45"
      },
      "source": [
        "#### 2.3) Deeper CNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Conv2D (32, 3×3) + BN + MaxPool + Dropout** → 320 + 64 params  \n",
        "- **Conv2D (64, 3×3) + BN + MaxPool + Dropout** → 18,496 + 128 params  \n",
        "- **Conv2D (128, 3×3) + BN + MaxPool + Dropout** → 73,856 + 256 params  \n",
        "- **Flatten**  \n",
        "- **Dense (128, ReLU) + BN + Dropout** → 147,584 + 256 params  \n",
        "- **Dense (10, Softmax)** → 1,290 params  \n",
        "\n",
        "**Total params:** ~242K  "
      ],
      "metadata": {
        "id": "sfelhQFFeYjZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        },
        "id": "7c6a1e85",
        "outputId": "e1322fea-b0b1-45d0-ac77-a27544176949"
      },
      "source": [
        "deeper_cnn_model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Dropout(0.25),\n",
        "\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Dropout(0.25),\n",
        "\n",
        "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Dropout(0.25),\n",
        "\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.5),\n",
        "\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "deeper_cnn_model.compile(optimizer='adam',\n",
        "                         loss='categorical_crossentropy',\n",
        "                         metrics=['accuracy'])\n",
        "\n",
        "deeper_cnn_model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m111,882\u001b[0m (437.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">111,882</span> (437.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m111,178\u001b[0m (434.29 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">111,178</span> (434.29 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m704\u001b[0m (2.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> (2.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffb65dc0"
      },
      "source": [
        "### 3. Train models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1) Training ANN Model"
      ],
      "metadata": {
        "id": "stP2wbh_XQE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **EarlyStopping**: monitors validation loss, stops if no improvement for 5 epochs, restores best weights.  \n",
        "- **ModelCheckpoint**: saves best ANN weights (`best_ann_model_weights.weights.h5`) based on validation loss.  \n",
        "- **Training**:  \n",
        "  - Epochs: up to 30 (early stopping controls actual run)  \n",
        "  - Batch size: 64  \n",
        "  - Validation on test set  \n",
        "  - Callbacks: EarlyStopping + ModelCheckpoint  "
      ],
      "metadata": {
        "id": "NhaurqFKd8zI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VJTWjvvQevB",
        "outputId": "4dc6efa8-ab0c-4d06-c917-dd60ae0d325a"
      },
      "source": [
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "model_checkpoint_ann = keras.callbacks.ModelCheckpoint(\n",
        "    filepath='best_ann_model_weights.weights.h5',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Training ANN model...\")\n",
        "ann_history = ann_model.fit(\n",
        "    train_images,\n",
        "    train_labels_one_hot,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    validation_data=(test_images, test_labels_one_hot),\n",
        "    callbacks=[early_stopping, model_checkpoint_ann]\n",
        ")\n",
        "print(\"ANN model training finished.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ANN model...\n",
            "Epoch 1/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7620 - loss: 0.6850\n",
            "Epoch 1: val_loss improved from inf to 0.42016, saving model to best_ann_model_weights.weights.h5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7620 - loss: 0.6848 - val_accuracy: 0.8522 - val_loss: 0.4202\n",
            "Epoch 2/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8618 - loss: 0.3819\n",
            "Epoch 2: val_loss improved from 0.42016 to 0.39626, saving model to best_ann_model_weights.weights.h5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8618 - loss: 0.3819 - val_accuracy: 0.8570 - val_loss: 0.3963\n",
            "Epoch 3/30\n",
            "\u001b[1m919/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8773 - loss: 0.3332\n",
            "Epoch 3: val_loss improved from 0.39626 to 0.35874, saving model to best_ann_model_weights.weights.h5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8773 - loss: 0.3332 - val_accuracy: 0.8741 - val_loss: 0.3587\n",
            "Epoch 4/30\n",
            "\u001b[1m932/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8863 - loss: 0.3104\n",
            "Epoch 4: val_loss did not improve from 0.35874\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8863 - loss: 0.3104 - val_accuracy: 0.8673 - val_loss: 0.3707\n",
            "Epoch 5/30\n",
            "\u001b[1m937/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8909 - loss: 0.2965\n",
            "Epoch 5: val_loss improved from 0.35874 to 0.34520, saving model to best_ann_model_weights.weights.h5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8909 - loss: 0.2965 - val_accuracy: 0.8748 - val_loss: 0.3452\n",
            "Epoch 6/30\n",
            "\u001b[1m932/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8975 - loss: 0.2776\n",
            "Epoch 6: val_loss improved from 0.34520 to 0.34171, saving model to best_ann_model_weights.weights.h5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8975 - loss: 0.2777 - val_accuracy: 0.8771 - val_loss: 0.3417\n",
            "Epoch 7/30\n",
            "\u001b[1m921/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8984 - loss: 0.2712\n",
            "Epoch 7: val_loss did not improve from 0.34171\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8984 - loss: 0.2711 - val_accuracy: 0.8716 - val_loss: 0.3479\n",
            "Epoch 8/30\n",
            "\u001b[1m917/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9050 - loss: 0.2546\n",
            "Epoch 8: val_loss did not improve from 0.34171\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9049 - loss: 0.2547 - val_accuracy: 0.8750 - val_loss: 0.3584\n",
            "Epoch 9/30\n",
            "\u001b[1m921/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9085 - loss: 0.2493\n",
            "Epoch 9: val_loss improved from 0.34171 to 0.32800, saving model to best_ann_model_weights.weights.h5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9084 - loss: 0.2493 - val_accuracy: 0.8831 - val_loss: 0.3280\n",
            "Epoch 10/30\n",
            "\u001b[1m820/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9091 - loss: 0.2378"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2) Training Basic CNN Model"
      ],
      "metadata": {
        "id": "OeTKHRl3YBmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ModelCheckpoint**: saves best Basic CNN weights (`best_basic_cnn_model_weights.weights.h5`) based on validation loss.  \n",
        "- **Training**:  \n",
        "  - Epochs: up to 30 (early stopping controls actual run)  \n",
        "  - Batch size: 64  \n",
        "  - Validation on test set  \n",
        "  - Callbacks: EarlyStopping + ModelCheckpoint"
      ],
      "metadata": {
        "id": "0ZbylWgGd_7M"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "979a9980"
      },
      "source": [
        "model_checkpoint_basic_cnn = keras.callbacks.ModelCheckpoint(\n",
        "    filepath='best_basic_cnn_model_weights.weights.h5',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Training Basic CNN model...\")\n",
        "basic_cnn_history = basic_cnn_model.fit(\n",
        "    train_images,\n",
        "    train_labels_one_hot,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    validation_data=(test_images, test_labels_one_hot),\n",
        "    callbacks=[early_stopping, model_checkpoint_basic_cnn]\n",
        ")\n",
        "print(\"Basic CNN model training finished.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3566a954"
      },
      "source": [
        "#### 3.3) Training Deeper CNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ModelCheckpoint**: saves best Deeper CNN weights (`best_deeper_cnn_model_weights.weights.h5`) based on validation loss.  \n",
        "- **Training**:  \n",
        "  - Epochs: up to 30 (early stopping controls actual run)  \n",
        "  - Batch size: 64  \n",
        "  - Validation on test set  \n",
        "  - Callbacks: EarlyStopping + ModelCheckpoint  "
      ],
      "metadata": {
        "id": "qtsdg-QXePFY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f23eb27"
      },
      "source": [
        "model_checkpoint_deeper_cnn = keras.callbacks.ModelCheckpoint(\n",
        "    filepath='best_deeper_cnn_model_weights.weights.h5',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Training Deeper CNN model...\")\n",
        "deeper_cnn_history = deeper_cnn_model.fit(\n",
        "    train_images,\n",
        "    train_labels_one_hot,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    validation_data=(test_images, test_labels_one_hot),\n",
        "    callbacks=[early_stopping, model_checkpoint_deeper_cnn]\n",
        ")\n",
        "print(\"Deeper CNN model training finished.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Model Evaluation"
      ],
      "metadata": {
        "id": "ZPXbOARBlCrg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1) Model Evaluation Code Explanation\n",
        "\n",
        "Import utilities → Load weights (ANN, Basic CNN, Deeper CNN) → Evaluate on test set → Get loss & accuracy → Print results"
      ],
      "metadata": {
        "id": "p7VnenQV00J-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80c0f0b0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "ann_model.load_weights('best_ann_model_weights.weights.h5')\n",
        "basic_cnn_model.load_weights('best_basic_cnn_model_weights.weights.h5')\n",
        "deeper_cnn_model.load_weights('best_deeper_cnn_model_weights.weights.h5')\n",
        "\n",
        "loss_ann, accuracy_ann = ann_model.evaluate(test_images, test_labels_one_hot, verbose=0)\n",
        "loss_basic_cnn, accuracy_basic_cnn = basic_cnn_model.evaluate(test_images, test_labels_one_hot, verbose=0)\n",
        "loss_deeper_cnn, accuracy_deeper_cnn = deeper_cnn_model.evaluate(test_images, test_labels_one_hot, verbose=0)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "models = ['ANN', 'Basic CNN', 'Deeper CNN']\n",
        "losses = [loss_ann, loss_basic_cnn, loss_deeper_cnn]\n",
        "accuracies = [accuracy_ann, accuracy_basic_cnn, accuracy_deeper_cnn]\n",
        "\n",
        "# Dark mode style\n",
        "plt.style.use(\"dark_background\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Loss plot\n",
        "axes[0].bar(models, losses, color='#FF6F61', edgecolor='white')\n",
        "axes[0].set_title('Model Test Loss', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('Loss', fontsize=12)\n",
        "axes[0].grid(True, linestyle='--', alpha=0.4)\n",
        "\n",
        "# Accuracy plot\n",
        "axes[1].bar(models, accuracies, color='#6BCB77', edgecolor='white')\n",
        "axes[1].set_title('Model Test Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
        "axes[1].grid(True, linestyle='--', alpha=0.4)\n",
        "\n",
        "# Overall title\n",
        "plt.suptitle(\"✨ Model Performance Comparison ✨\", fontsize=16, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2) Model Evaluation (Confusion Matrix & Report)\n",
        "\n",
        "- Predict test images (ANN, Basic CNN, Deeper CNN)  \n",
        "- Convert one-hot labels to integers  \n",
        "- Get predicted labels with `argmax`  \n",
        "- Print confusion matrix & classification report for each model\n"
      ],
      "metadata": {
        "id": "Kfxk-yy71euJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6caea998"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Predictions\n",
        "ann_predictions = ann_model.predict(test_images)\n",
        "basic_cnn_predictions = basic_cnn_model.predict(test_images)\n",
        "deeper_cnn_predictions = deeper_cnn_model.predict(test_images)\n",
        "\n",
        "# Convert labels\n",
        "test_labels_int = np.argmax(test_labels_one_hot, axis=1)\n",
        "\n",
        "ann_predicted_labels = np.argmax(ann_predictions, axis=1)\n",
        "basic_cnn_predicted_labels = np.argmax(basic_cnn_predictions, axis=1)\n",
        "deeper_cnn_predicted_labels = np.argmax(deeper_cnn_predictions, axis=1)\n",
        "\n",
        "# Confusion matrices\n",
        "cm_ann = confusion_matrix(test_labels_int, ann_predicted_labels)\n",
        "cm_basic = confusion_matrix(test_labels_int, basic_cnn_predicted_labels)\n",
        "cm_deeper = confusion_matrix(test_labels_int, deeper_cnn_predicted_labels)\n",
        "\n",
        "# Plot side by side\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "sns.heatmap(cm_ann, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[0])\n",
        "axes[0].set_title(\"ANN Model\")\n",
        "axes[0].set_xlabel(\"Predicted\")\n",
        "axes[0].set_ylabel(\"Actual\")\n",
        "\n",
        "sns.heatmap(cm_basic, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[1])\n",
        "axes[1].set_title(\"Basic CNN Model\")\n",
        "axes[1].set_xlabel(\"Predicted\")\n",
        "axes[1].set_ylabel(\"Actual\")\n",
        "\n",
        "sns.heatmap(cm_deeper, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[2])\n",
        "axes[2].set_title(\"Deeper CNN Model\")\n",
        "axes[2].set_xlabel(\"Predicted\")\n",
        "axes[2].set_ylabel(\"Actual\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.3) Model Comparison\n",
        "\n",
        "- Created a dataframe to store **test loss** and **test accuracy** for ANN, Basic CNN, and Deeper CNN.  \n",
        "- Plotted training history for each model (accuracy & loss vs. epochs).  \n",
        "- Helps compare how models learn and generalize over time.  "
      ],
      "metadata": {
        "id": "Z_96B2DvnRe2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c8abd42"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "models = {\n",
        "    \"ANN\": (\"cyan\", ann_history),\n",
        "    \"Basic CNN\": (\"magenta\", basic_cnn_history),\n",
        "    \"Deeper CNN\": (\"yellow\", deeper_cnn_history)\n",
        "}\n",
        "\n",
        "performance_data = {\n",
        "    'Model': ['ANN', 'Basic CNN', 'Deeper CNN'],\n",
        "    'Test Loss': [loss_ann, loss_basic_cnn, loss_deeper_cnn],\n",
        "    'Test Accuracy': [accuracy_ann, accuracy_basic_cnn, accuracy_deeper_cnn]\n",
        "}\n",
        "\n",
        "performance_df = pd.DataFrame(performance_data)\n",
        "\n",
        "fig = make_subplots(rows=1, cols=2, subplot_titles=('Accuracy', 'Loss'))\n",
        "\n",
        "for name, (color, history) in models.items():\n",
        "    # Accuracy\n",
        "    fig.add_trace(go.Scatter(y=history.history['accuracy'], mode='lines',\n",
        "                             name=name, line=dict(color=color, dash='solid')), row=1, col=1)\n",
        "    fig.add_trace(go.Scatter(y=history.history['val_accuracy'], mode='lines',\n",
        "                             name=name, line=dict(color=color, dash='dot'), showlegend=False), row=1, col=1)\n",
        "\n",
        "    # Loss\n",
        "    fig.add_trace(go.Scatter(y=history.history['loss'], mode='lines',\n",
        "                             name=name, line=dict(color=color, dash='solid')), row=1, col=2)\n",
        "    fig.add_trace(go.Scatter(y=history.history['val_loss'], mode='lines',\n",
        "                             name=name, line=dict(color=color, dash='dot'), showlegend=False), row=1, col=2)\n",
        "\n",
        "fig.update_layout(\n",
        "    title_text=\"Model Comparison: Accuracy & Loss (Train=Solid, Val=Dot)\",\n",
        "    height=500,\n",
        "    width=1200,\n",
        "    template=\"plotly_dark\"\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.4) Predictions\n",
        "\n",
        "This code:\n",
        "- Uses the trained **Basic CNN model** to predict on test images.  \n",
        "- Identifies **correctly** and **incorrectly** classified samples.  \n",
        "- Randomly selects 100 correct and 100 incorrect predictions.  \n",
        "- Displays them in a **20×10 grid**:  \n",
        "  - ✅ Correct predictions → **Green title** (only predicted label).  \n",
        "  - ❌ Incorrect predictions → **Red title** (Predicted vs True label).  \n"
      ],
      "metadata": {
        "id": "hxPlh1hN3T4M"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8beadb19"
      },
      "source": [
        "basic_cnn_predictions = basic_cnn_model.predict(test_images)\n",
        "basic_cnn_predicted_labels = np.argmax(basic_cnn_predictions, axis=1)\n",
        "\n",
        "# Find correctly and incorrectly classified images\n",
        "correctly_classified_indices = np.where(basic_cnn_predicted_labels == test_labels)[0]\n",
        "incorrectly_classified_indices = np.where(basic_cnn_predicted_labels != test_labels)[0]\n",
        "\n",
        "# Define class names for Fashion-MNIST\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Select a mix of correctly and incorrectly classified images\n",
        "num_correct_to_show = 100\n",
        "num_incorrect_to_show = 100\n",
        "\n",
        "# Ensure we don't try to show more images than available\n",
        "num_correct_to_show = min(num_correct_to_show, len(correctly_classified_indices))\n",
        "num_incorrect_to_show = min(num_incorrect_to_show, len(incorrectly_classified_indices))\n",
        "\n",
        "selected_correct_indices = np.random.choice(correctly_classified_indices, num_correct_to_show, replace=False)\n",
        "selected_incorrect_indices = np.random.choice(incorrectly_classified_indices, num_incorrect_to_show, replace=False)\n",
        "\n",
        "# Combine and shuffle the indices\n",
        "all_indices = np.concatenate((selected_correct_indices, selected_incorrect_indices))\n",
        "np.random.shuffle(all_indices)\n",
        "\n",
        "# Display images in a 20x10 grid\n",
        "num_cols = 20\n",
        "num_rows = 10\n",
        "num_images_to_show = num_correct_to_show + num_incorrect_to_show\n",
        "\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(2 * num_cols, 2 * num_rows))\n",
        "fig.suptitle(\"Basic CNN Predictions (Mix of Correct and Incorrect)\", fontsize=18, y=1.02)\n",
        "\n",
        "for i, idx in enumerate(all_indices[:num_images_to_show]):\n",
        "    row = i // num_cols\n",
        "    col = i % num_cols\n",
        "    axes[row, col].imshow(test_images[idx].reshape(28, 28), cmap='gray')\n",
        "\n",
        "    pred = class_names[basic_cnn_predicted_labels[idx]]\n",
        "    true = class_names[test_labels[idx]]\n",
        "\n",
        "    if basic_cnn_predicted_labels[idx] == test_labels[idx]:\n",
        "        color = \"green\"\n",
        "        title = f\"{pred}\"\n",
        "    else:\n",
        "        color = \"red\"\n",
        "        title = f\"P:{pred}\\nT:{true}\"\n",
        "\n",
        "    axes[row, col].set_title(title, fontsize=8, color=color)\n",
        "    axes[row, col].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4917e428"
      },
      "source": [
        "## Project Summary and Conclusion\n",
        "\n",
        "This project successfully implemented and compared three different deep learning models (ANN, Basic CNN, and Deeper CNN) for classifying images from the Fashion-MNIST dataset.\n",
        "\n",
        "**Summary of Work:**\n",
        "\n",
        "1.  **Data Preparation:** The Fashion-MNIST dataset was loaded, normalized, reshaped, and one-hot encoded, preparing it for use with the different model architectures.\n",
        "2.  **Model Development:** Three models of increasing complexity were defined: a basic Artificial Neural Network (ANN), a basic Convolutional Neural Network (CNN), and a deeper CNN with additional layers, batch normalization, and dropout.\n",
        "3.  **Model Training:** Each model was trained using the preprocessed training data with Early Stopping and Model Checkpointing to prevent overfitting and save the best performing weights based on validation loss.\n",
        "4.  **Model Evaluation:** The trained models were evaluated on the test set. Performance metrics (loss and accuracy) were calculated, and the training history was visualized. Confusion matrices were generated to analyze class-specific performance.\n",
        "5.  **Prediction Analysis:** Predictions were made using the Basic CNN model, and examples of correctly and incorrectly classified images were visualized to gain insights into the model's strengths and weaknesses.\n",
        "\n",
        "**Key Findings and Conclusion:**\n",
        "\n",
        "Based on the evaluation results:\n",
        "\n",
        "*   The **Basic CNN model** generally achieved the best balance of performance (highest accuracy, lowest loss) on the test set compared to the ANN and Deeper CNN models.\n",
        "*   The **ANN model** performed reasonably well but was outperformed by both CNN architectures, highlighting the advantage of convolutional layers for image classification tasks.\n",
        "*   The **Deeper CNN model**, despite its increased complexity, did not consistently outperform the Basic CNN model on this dataset. This could be due to various factors such as the dataset size, the architecture choices, or the regularization applied. For this particular task and dataset, the increased complexity of the deeper model might not have been necessary or could have led to some overfitting despite the regularization techniques.\n",
        "\n",
        "In conclusion, the Basic CNN model demonstrated superior performance for this Fashion-MNIST classification task, suggesting that a moderate level of complexity with convolutional layers is effective for this dataset. Further tuning of hyperparameters or architectural variations might potentially improve performance across all models, but the current results clearly show the benefits of CNNs over ANNs for image data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "716b538e"
      },
      "source": [
        "## Assignment: CIFAR-100 Image Classification\n",
        "\n",
        "Your task is to apply the concepts and techniques learned in this Fashion-MNIST project to the CIFAR-100 dataset. CIFAR-100 is a dataset consisting of 60,000 32x32 color images in 100 classes, with 600 images per class. There are 50,000 training images and 10,000 test images.\n",
        "\n",
        "Follow these steps:\n",
        "\n",
        "1.  **Dataset Setup:**\n",
        "    *   Load the CIFAR-100 dataset.\n",
        "    *   Preprocess the data (normalize pixel values, one-hot encode labels). Remember that CIFAR-100 images are 32x32 color images, so the input shape will be different from Fashion-MNIST.\n",
        "    *   Verify the shapes of the processed data.\n",
        "\n",
        "2.  **Model Building:**\n",
        "    *   Adapt the ANN and CNN model architectures for the CIFAR-100 dataset. Consider that CIFAR-100 images are color (3 channels) and have a different resolution (32x32). You might need to adjust the input layer and potentially the number of filters or layers in the CNNs for better performance on a more complex dataset.\n",
        "\n",
        "3.  **Model Training:**\n",
        "    *   Train the models using the preprocessed CIFAR-100 training data. Use Early Stopping and Model Checkpointing as implemented before.\n",
        "\n",
        "4.  **Model Evaluation:**\n",
        "    *   Evaluate the trained models on the CIFAR-100 test set. Compare their performance using loss and accuracy.\n",
        "    *   Visualize training history and confusion matrices.\n",
        "\n",
        "5.  **Prediction Analysis:**\n",
        "    *   Choose the best performing model and analyze its predictions on the CIFAR-100 test set.\n",
        "\n",
        "**Goal:** To understand how model complexity and architecture choices impact performance on a more challenging image classification dataset like CIFAR-100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfc37839"
      },
      "source": [
        "import keras\n",
        "\n",
        "(cifar100_train_images, cifar100_train_labels), (cifar100_test_images, cifar100_test_labels) = keras.datasets.cifar100.load_data()\n",
        "\n",
        "print(\"CIFAR-100 Training images shape:\", cifar100_train_images.shape)\n",
        "print(\"CIFAR-100 Testing images shape:\", cifar100_test_images.shape)\n",
        "print(\"CIFAR-100 Training labels shape:\", cifar100_train_labels.shape)\n",
        "print(\"CIFAR-100 Testing labels shape:\", cifar100_test_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dataset Setup  \n",
        "\n",
        "In this step, we load and preprocess the **CIFAR-100 dataset**.  \n",
        "This dataset contains **60,000 images** (50,000 training + 10,000 testing), each of size **32×32 pixels with 3 color channels (RGB)**, divided into **100 fine-grained classes** (e.g., apple, bus, dog, etc.).  \n",
        "\n",
        "**Steps performed:**  \n",
        "- Load dataset using `keras.datasets.cifar100`  \n",
        "- Normalize pixel values (0–255 → 0–1) for faster convergence  \n",
        "- One-hot encode labels into 100 classes"
      ],
      "metadata": {
        "id": "IbrApFMD50m-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "(cifar100_train_images, cifar100_train_labels), (cifar100_test_images, cifar100_test_labels) = keras.datasets.cifar100.load_data()\n",
        "\n",
        "print(\"CIFAR-100 Training images shape:\", cifar100_train_images.shape)\n",
        "print(\"CIFAR-100 Testing images shape:\", cifar100_test_images.shape)\n",
        "print(\"CIFAR-100 Training labels shape:\", cifar100_train_labels.shape)\n",
        "print(\"CIFAR-100 Testing labels shape:\", cifar100_test_labels.shape)\n",
        "\n",
        "# Normalize images (0–255 → 0–1)\n",
        "cifar100_train_images = cifar100_train_images.astype(\"float32\") / 255.0\n",
        "cifar100_test_images = cifar100_test_images.astype(\"float32\") / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "cifar100_train_labels = keras.utils.to_categorical(cifar100_train_labels, 100)\n",
        "cifar100_test_labels = keras.utils.to_categorical(cifar100_test_labels, 100)\n"
      ],
      "metadata": {
        "id": "InGCysoH51RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. CNN Model Building  \n",
        "\n",
        "We design a **Convolutional Neural Network (CNN)** suitable for CIFAR-100 classification.  \n",
        "\n",
        "**Architecture:**  \n",
        "- `Conv2D (32 filters, 3×3, ReLU)` → feature extraction  \n",
        "- `MaxPooling2D (2×2)` → downsampling  \n",
        "- `Conv2D (64 filters, 3×3, ReLU)` → deeper feature maps  \n",
        "- `MaxPooling2D (2×2)` → downsampling  \n",
        "- `Conv2D (128 filters, 3×3, ReLU)` → high-level features  \n",
        "- `MaxPooling2D (2×2)` → downsampling  \n",
        "- `Flatten` → convert 2D features to 1D  \n",
        "- `Dense (256, ReLU)` → fully connected layer  \n",
        "- `Dropout (0.5)` → reduce overfitting  \n",
        "- `Dense (100, Softmax)` → final classification layer  \n",
        "\n",
        "**Compilation:**  \n",
        "- Optimizer: **Adam**  \n",
        "- Loss: **Categorical Crossentropy** (multi-class)  \n",
        "- Metric: **Accuracy**  "
      ],
      "metadata": {
        "id": "UYe6xnSj556G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation=\"relu\", padding=\"same\", input_shape=(32,32,3)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), activation=\"relu\", padding=\"same\"),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(128, (3,3), activation=\"relu\", padding=\"same\"),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation=\"relu\"),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(100, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "6pDVkYU-56W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Callbacks (EarlyStopping + ModelCheckpoint)  \n",
        "\n",
        "To improve training stability and efficiency:  \n",
        "- **EarlyStopping:** Stops training if validation loss does not improve for 5 epochs (avoids overfitting).  \n",
        "- **ModelCheckpoint:** Saves the best model weights whenever validation accuracy improves.  \n"
      ],
      "metadata": {
        "id": "80kBoDSb57tJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "checkpoint_cb = ModelCheckpoint(\"best_cifar100_model.h5\", save_best_only=True)\n",
        "earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n"
      ],
      "metadata": {
        "id": "5FOghG6f58Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training the Model  \n",
        "\n",
        "The CNN is trained with the following setup:  \n",
        "- **Epochs:** 30  \n",
        "- **Batch size:** 64  \n",
        "- **Validation split:** 20% of training data used for validation  \n",
        "- **Callbacks:** EarlyStopping + ModelCheckpoint  \n",
        "\n",
        "These settings ensure we do not overtrain and always keep the best version of the model.  \n"
      ],
      "metadata": {
        "id": "Q8ZgI6kh59ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    cifar100_train_images, cifar100_train_labels,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[checkpoint_cb, earlystop_cb]\n",
        ")\n"
      ],
      "metadata": {
        "id": "XOUXy4wx5-Ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model Evaluation  \n",
        "\n",
        "After training is complete:  \n",
        "- Load the **best saved weights**  \n",
        "- Evaluate the model on the **test dataset (10,000 images)**  \n",
        "- Report the **final test accuracy and loss**  \n",
        "\n",
        "This evaluation shows how well the model generalizes to unseen data.  "
      ],
      "metadata": {
        "id": "WSIqRVT-5_t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best weights\n",
        "model.load_weights(\"best_cifar100_model.h5\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(cifar100_test_images, cifar100_test_labels, verbose=2)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "ozQxE4N96ATW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Training Visualization  \n",
        "\n",
        "We visualize training progress using two plots:  \n",
        "- **Accuracy curve** → compares training vs validation accuracy per epoch  \n",
        "- **Loss curve** → compares training vs validation loss per epoch  \n",
        "\n",
        "These plots help diagnose:  \n",
        "- Overfitting (training accuracy high, validation accuracy low)  \n",
        "- Underfitting (both accuracies low)  \n",
        "- Good fit (both accuracies high and close together)  \n"
      ],
      "metadata": {
        "id": "sCuZ0-SU6FGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title(\"Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5-nquBR66FgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Confusion Matrix  \n",
        "\n",
        "A **confusion matrix** helps analyze class-wise performance.  \n",
        "- Rows → True labels  \n",
        "- Columns → Predicted labels  \n",
        "- Diagonal → Correct classifications  \n",
        "\n",
        "Due to 100 classes, we may visualize only a subset (e.g., first 20×20).  \n"
      ],
      "metadata": {
        "id": "yUIH62W56HnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Predict labels for test set\n",
        "y_pred_probs = model.predict(cifar100_test_images)\n",
        "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "y_true_classes = np.argmax(cifar100_test_labels, axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(cm[:20,:20], annot=False, cmap=\"Blues\")  # show only first 20x20 for clarity\n",
        "plt.title(\"Confusion Matrix (First 20 Classes)\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "U-8p24cK6H-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Sample Prediction Visualization  \n",
        "\n",
        "We visualize model predictions on random test samples:  \n",
        "- Display a grid of test images  \n",
        "- Show **true label vs predicted label**  \n",
        "- Use **green text** if prediction is correct, **red text** if incorrect  \n",
        "\n",
        "This helps us qualitatively assess how the model performs across different categories.  \n"
      ],
      "metadata": {
        "id": "Hlxm2ZNr6Zl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR-100 class names\n",
        "cifar100_labels = keras.datasets.cifar100.load_data(label_mode=\"fine\")[1]\n",
        "class_names = [\n",
        "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee',\n",
        "    'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus',\n",
        "    'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle',\n",
        "    # ... (100 classes total, can be loaded from keras directly)\n",
        "]\n",
        "\n",
        "# Plot a few test images with predictions\n",
        "plt.figure(figsize=(12,12))\n",
        "for i in range(16):\n",
        "    idx = np.random.randint(0, len(cifar100_test_images))\n",
        "    img = cifar100_test_images[idx]\n",
        "    true_label = np.argmax(cifar100_test_labels[idx])\n",
        "    pred_label = y_pred_classes[idx]\n",
        "\n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"True: {true_label}\\nPred: {pred_label}\",\n",
        "              color=(\"green\" if true_label==pred_label else \"red\"))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_n9p-CS66Z-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Summary  \n",
        "\n",
        "This notebook demonstrates:  \n",
        "- CIFAR-100 dataset preprocessing  \n",
        "- CNN model design for image classification  \n",
        "- Training with callbacks (EarlyStopping + ModelCheckpoint)  \n",
        "- Performance evaluation and visualization  \n",
        "- Confusion matrix and prediction examples  \n",
        "\n",
        "The workflow ensures a clear understanding of **how CNNs handle multi-class image classification** on a challenging dataset like CIFAR-100.  "
      ],
      "metadata": {
        "id": "l6kYGXpyY8eR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2B. ANN Model for CIFAR-100\n",
        "\n",
        "While CNNs are best suited for image data, we’ll also implement a simple Artificial Neural Network (ANN) for comparison.  \n",
        "The ANN will flatten the image pixels and feed them into fully connected dense layers.\n"
      ],
      "metadata": {
        "id": "tHIhoxyAZY3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ANN Model\n",
        "ann_model = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(32,32,3)),          # Flatten 32x32x3 = 3072 inputs\n",
        "    layers.Dense(512, activation=\"relu\"),           # First hidden layer\n",
        "    layers.Dense(256, activation=\"relu\"),           # Second hidden layer\n",
        "    layers.Dropout(0.5),                            # Dropout for regularization\n",
        "    layers.Dense(100, activation=\"softmax\")         # Output layer for 100 classes\n",
        "])\n",
        "\n",
        "ann_model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "ann_model.summary()\n"
      ],
      "metadata": {
        "id": "N8Dqh9XWZZd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3B. Training the ANN"
      ],
      "metadata": {
        "id": "Zf6jkPyAZnqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ANN with early stopping & checkpoint\n",
        "ann_checkpoint_cb = ModelCheckpoint(\"best_ann_cifar100.h5\", save_best_only=True)\n",
        "ann_earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n",
        "\n",
        "ann_history = ann_model.fit(\n",
        "    cifar100_train_images, cifar100_train_labels,\n",
        "    epochs=30,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[ann_checkpoint_cb, ann_earlystop_cb]\n",
        ")\n"
      ],
      "metadata": {
        "id": "4mnHtIVoZ5Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4B. ANN Evaluation"
      ],
      "metadata": {
        "id": "q1pQCWn8Zv0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best weights\n",
        "ann_model.load_weights(\"best_ann_cifar100.h5\")\n",
        "\n",
        "# Evaluate\n",
        "ann_test_loss, ann_test_acc = ann_model.evaluate(cifar100_test_images, cifar100_test_labels, verbose=2)\n",
        "print(f\"ANN Test Accuracy: {ann_test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "bXEtoqsfZ9jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6B. ANN Training Visualization"
      ],
      "metadata": {
        "id": "Tkgcc8KVaAVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(ann_history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(ann_history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title(\"ANN Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(ann_history.history['loss'], label='Train Loss')\n",
        "plt.plot(ann_history.history['val_loss'], label='Val Loss')\n",
        "plt.title(\"ANN Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "M8gTX8cAaBfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔍 Comparison: ANN vs CNN\n",
        "\n",
        "- **ANN**  \n",
        "  * Treats images as flat vectors (ignores spatial features).  \n",
        "  * Typically gets **~20–30% accuracy** on CIFAR-100.  \n",
        "\n",
        "- **CNN**  \n",
        "  * Learns spatial patterns (edges, textures, shapes).  \n",
        "  * Performs much better (**40–60%+ accuracy**) even with a simple architecture.  \n",
        "\n",
        "👉 This demonstrates why **CNNs are the preferred choice** for complex image datasets like CIFAR-100.\n"
      ],
      "metadata": {
        "id": "KTp9uh0FaGIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📊 ANN vs CNN Accuracy Comparison\n",
        "\n",
        "To better visualize the performance gap between ANN and CNN on CIFAR-100, we plot their test accuracies side by side.\n"
      ],
      "metadata": {
        "id": "Nj0Fcz4eaJOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Replace with your actual evaluated test accuracies\n",
        "ann_acc = ann_test_acc\n",
        "cnn_acc = test_acc\n",
        "\n",
        "models = [\"ANN\", \"CNN\"]\n",
        "accuracies = [ann_acc, cnn_acc]\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "bars = plt.bar(models, accuracies, color=[\"orange\", \"skyblue\"])\n",
        "\n",
        "# Add value labels on top of bars\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, f\"{yval:.2f}\",\n",
        "             ha=\"center\", va=\"bottom\", fontsize=12, fontweight=\"bold\")\n",
        "\n",
        "plt.ylim(0, 1)  # accuracy between 0 and 1\n",
        "plt.ylabel(\"Test Accuracy\")\n",
        "plt.title(\"ANN vs CNN Accuracy on CIFAR-100\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2jKUGYtiaKx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📊 ANN vs CNN Accuracy Comparison\n",
        "\n",
        "To better visualize the performance gap between ANN and CNN on CIFAR-100, we present both a **comparison table** and a **bar chart**.\n",
        "\n",
        "### 🔹 Comparison Table\n",
        "\n",
        "| Model | Test Accuracy |\n",
        "|-------|---------------|\n",
        "| ANN   | ~0.20–0.30    |\n",
        "| CNN   | ~0.40–0.60+   |\n",
        "\n",
        "👉 Clearly, the CNN significantly outperforms the ANN by leveraging spatial feature learning.\n",
        "\n",
        "### 🔹 Bar Chart\n"
      ],
      "metadata": {
        "id": "rFRp8vXAaQzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Replace with your actual evaluated test accuracies\n",
        "ann_acc = ann_test_acc\n",
        "cnn_acc = test_acc\n",
        "\n",
        "models = [\"ANN\", \"CNN\"]\n",
        "accuracies = [ann_acc, cnn_acc]\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "bars = plt.bar(models, accuracies, color=[\"orange\", \"skyblue\"])\n",
        "\n",
        "# Add value labels on top of bars\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, f\"{yval:.2f}\",\n",
        "             ha=\"center\", va=\"bottom\", fontsize=12, fontweight=\"bold\")\n",
        "\n",
        "plt.ylim(0, 1)  # accuracy between 0 and 1\n",
        "plt.ylabel(\"Test Accuracy\")\n",
        "plt.title(\"ANN vs CNN Accuracy on CIFAR-100\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "widJeYypaRow"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}